{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605106db-1f86-491e-8775-3172480ba821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f2e953-c65a-42b4-939b-73b0e9c75277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3000cbe-785c-4415-bc92-d91f6c1a64de",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eef3dce-1838-4212-bb0d-c02ede1957e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc42eec5168d4b03811cbca9db0aaab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46edc83-8f4d-4a63-a455-23d6a9248db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.347264\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7261433-31d7-46dc-8040-dff0703c8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08062e-695e-487d-bc4d-7c0277970f96",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b5b64c-9af3-418f-9c2e-a64c7d20b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193dba94-b3d0-4e1b-a856-4b24154f1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98bd14ca-fc9d-4558-ae5d-bc5c338eb4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5ff19aba0f4795ba3556d61ca46e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_chat_template(sample):\n",
    "    sample['messages'] = [\n",
    "        {\"role\":\"user\", \"content\":sample['prompt']},\n",
    "        {\"role\":\"assistant\", \"content\":sample['completion']}]\n",
    "    del sample['prompt']\n",
    "    del sample['completion']\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(apply_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa69a3-ae20-4f38-9ed6-8d6827e4131a",
   "metadata": {},
   "source": [
    "# Ensure Dataset is in Conversational Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7acb3f2-babe-4666-a92d-56a7560af38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c32de746f544219d651179e457de88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'fn_kwargs'={'tokenizer': LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564c4803b77a4016acef6e389065772e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2a957864d2409e8af65baa40997148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ca322451d84c70b78f7a3fc3b16d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run SFTTrainer._prepare_dataset on the training data\n",
    "model_name = model.config._name_or_path.split(\"/\")[-1]\n",
    "args = SFTConfig(f\"{model_name}-SFT\")\n",
    "processed_dataset = SFTTrainer._prepare_dataset(SFTTrainer, dataset, tokenizer, args, args.packing, None, \"train\")\n",
    "\n",
    "# The output should be in the conversation template.\n",
    "processed_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd593c-e1fb-41ef-aea4-373e3ac3eed7",
   "metadata": {},
   "source": [
    "# Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b11524a-9055-4e5d-8146-ed1f8098d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,                   # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
    "    lora_alpha=16,         # multiplier, usually 2*r\n",
    "    bias=\"none\",           # BEWARE: training biases *modifies* base model's behavior\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Newer models, such as Phi-3 at time of writing, may require\n",
    "    # manually setting target modules\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d855b46-4dc9-4180-9fe0-b2df19301b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.080704\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0946c08-0368-4a28-b6a1-4b85ea56b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:             12.58M\n",
      "Total parameters:                 3833.66M\n",
      "Fraction of trainable parameters: 0.33%\n"
     ]
    }
   ],
   "source": [
    "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
    "print(f'Trainable parameters:             {trainable_parms/1e6:.2f}M')\n",
    "print(f'Total parameters:                 {tot_parms/1e6:.2f}M')\n",
    "print(f'Fraction of trainable parameters: {100*trainable_parms/tot_parms:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be91a1-ef0a-4319-8d8d-b5060326fa5d",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3dca96-0896-4cbd-bfb2-a8e0bb292b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,\n",
    "    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,\n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=16,\n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "    \n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=64,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=False,\n",
    "    \n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    optim='paged_adamw_8bit',\n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f23f71-43ce-4fee-aac8-f5b1cd4dc02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f62dd963481457289122d1ec44f2cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da309cb70f7c4080873b93025b8557a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dea7f3e8bd4d9da1ae550b50e15cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd9213b1-7013-4f40-9036-641d05de27e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|> They took their kids from the public school.<|end|><|assistant|> From the public school, their kids they took. Hrmmm.<|end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4af9074-bfe6-4003-98f8-0c66685ad84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 03:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.975400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=135, training_loss=1.2666392467640064, metrics={'train_runtime': 236.5761, 'train_samples_per_second': 9.13, 'train_steps_per_second': 0.571, 'total_flos': 1873198611431424.0, 'train_loss': 1.2666392467640064})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d1e02-5267-4f19-b335-96e5343415e3",
   "metadata": {},
   "source": [
    "# Query the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c01d094-7fdf-4deb-8f27-387b94141ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample, \n",
    "                                           tokenize=False, \n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generation_output = model.generate(**tokenized_input,\n",
    "                                       max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    output = tokenizer.batch_decode(generation_output, \n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a2ec0b-a40d-451a-bc98-04d999a5a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> The force is strong in you!<|end|><|assistant|> The phrase \"The force is strong in you!\" is a famous line from the Star Wars movie series, specifically from \"The Empire Strikes Back.\" It is spoken by Darth Vader to Luke Skywalker in the climactic scene where Luke is about to be frozen in carbonite. The line has\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(generate(merged_model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b82ac8-228e-4d90-b8f8-d5251cfe1e98",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8acd6af0-7aa9-4717-bbe5-12343d9551df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trainer.save_model('yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7c058-d05e-49c9-9d25-d56990540353",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db783b9e-e8f4-4053-bbe9-92be8edbce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152ac16fe619412e9ae7f292bf328854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = 'yoda-adapter'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cuda:0\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f5d3c8-28b9-441c-8ecf-101f4d12401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> The force is strong in you!<|end|><|assistant|> Strong in you, the force is!<|end|>\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
