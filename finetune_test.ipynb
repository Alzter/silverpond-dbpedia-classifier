{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized LLM Fine-Tuning Test (QLoRA)\n",
    "\n",
    "Source: https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "Phi-3 Mini 4K Instruct (3.8B parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# QLoRA quantisation configuration.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\", # QLoRA,\n",
    "    bnb_4bit_use_double_quant = True, # QLoRA,\n",
    "    bnb_4bit_compute_dtype = torch.float32\n",
    ")\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Mb of RAM is the model using?\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Low-Rank Adapters (LoRA)\n",
    "Low-rank adapters will be attached to every quantized layer.\n",
    "These adapters will be modifiable with training, but the layers they are attached to will be frozen.\n",
    "The adapters take up about 1% of the size of the original layers, which dramatically reduces the size of the fine-tuned model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "# Improve the numerical stability of our model during training\n",
    "# by turning every non-quantized layer to full precision (FP32)\n",
    "model = peft.prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = peft.LoraConfig(\n",
    "    # The rank of the adapter, or the number of trainable fine-tuning parameters.\n",
    "    r = 8,\n",
    "    lora_alpha = 16, # Multiplier, usually 2*r\n",
    "    bias = \"none\",\n",
    "    lora_dropout = 0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Newer models, such as Phi-3 at time of writing, may require \n",
    "    # manually setting target modules\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = peft.get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many Mb of RAM is the model using?\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the supervised dataset\n",
    "For this toy example we will be loading an English-to-Yoda-speak dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to SFTTrainer compatible form\n",
    "\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,  \n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=16, \n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=64,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    optim='paged_adamw_8bit',       \n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # Should take ~30m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_prompt(tokenizer, text):\n",
    "    \"\"\"\n",
    "        Assemble a message in the conversational format \n",
    "        and apply the chat template to it,\n",
    "        appending the generation prompt to its end.\n",
    "    \"\"\"\n",
    "    \n",
    "    converted_sample = [{\"role\":\"user\", \"content\":text}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens = 64, skip_special_tokens=False):\n",
    "    tokenized_input= tokenizer(\n",
    "        prompt, add_special_tokens=False, return_tensors = \"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval() # Don't modify my weights please\n",
    "\n",
    "    gen_output = model.generate(\n",
    "        **tokenized_input,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        max_new_tokens = max_new_tokens\n",
    "    )\n",
    "\n",
    "    output = tokenizer.batch_decode(\n",
    "        gen_output,\n",
    "        skip_special_tokens = skip_special_tokens,\n",
    "    )\n",
    "\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model, tokenizer,\n",
    "    generate_prompt(\"I must destroy your cheese.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"local-phi3-mini-yoda-adapter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
