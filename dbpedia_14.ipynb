{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 21 13:24:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:19:00.0 Off |                  N/A |\n",
      "| 27%   27C    P5              8W /  250W |     368MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8             21W /  250W |       1MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8             18W /  250W |       1MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:68:00.0  On |                  N/A |\n",
      "| 27%   31C    P8             21W /  250W |       9MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1819041      C   ...jl7hl-python3-3.12.8/bin/python3.12        182MiB |\n",
      "|    0   N/A  N/A   1864189      C   ...jl7hl-python3-3.12.8/bin/python3.12        182MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "torch.cuda.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKRSEolKGDiw"
   },
   "source": [
    "# Load DBPedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iCnfFuWPWr9-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company',\n",
       " 'EducationalInstitution',\n",
       " 'Artist',\n",
       " 'Athlete',\n",
       " 'OfficeHolder',\n",
       " 'MeanOfTransportation',\n",
       " 'Building',\n",
       " 'NaturalPlace',\n",
       " 'Village',\n",
       " 'Animal',\n",
       " 'Plant',\n",
       " 'Album',\n",
       " 'Film',\n",
       " 'WrittenWork']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"fancyzhx/dbpedia_14\")\n",
    "CLASS_LABELS = ds['train'].features['label'].names\n",
    "CLASS_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY442nBwWr-A"
   },
   "source": [
    "## Reduce dataset size via sampling\n",
    "Let's obtain the first **n** samples from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K4j8rSsCWr-D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_n_samples_per_class(dataset, n, shuffle = False):\n",
    "    \"\"\"\n",
    "        Given a dataset, obtain the first n samples from each class\n",
    "        and return a smaller dataset containing all the samples.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to sample.\n",
    "            n (int): How many samples from each class to extract.\n",
    "            shuffle (bool): Whether to sort the final result by class or randomly. NOTE: Dataset.shuffle() hangs indefinitely on Nix.\n",
    "\n",
    "        Returns:\n",
    "            sample (Dataset): The sampled dataset.\n",
    "    \"\"\"\n",
    "    ds_sorted = dataset.sort('label')\n",
    "    _, class_indices = np.unique(ds_sorted['label'], return_index=True)\n",
    "\n",
    "\n",
    "    class_indices = np.array([list(range(index, index + n)) for index in class_indices])\n",
    "    class_indices = class_indices.flatten()\n",
    "\n",
    "    if shuffle:\n",
    "        sample = dataset.shuffle().sort('label').select(class_indices) # Dataset.shuffle() hangs indefinitely on Nix - No idea why.\n",
    "    else:\n",
    "        sample = dataset.sort('label').select(class_indices)\n",
    "\n",
    "    if shuffle: sample = sample.shuffle() # Dataset.shuffle() hangs indefinitely on Nix - No idea why.\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proportion = 0.1 # What percentage of the dataset do you want to use?\n",
    "\n",
    "samples_per_class = int(40000 * dataset_proportion)\n",
    "\n",
    "ds['train'] = get_n_samples_per_class(ds['train'], samples_per_class, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mPbBqs74Wr-E"
   },
   "outputs": [],
   "source": [
    "ds['test'] = get_n_samples_per_class(ds['test'], 6, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format training data as a supervised fine-tuning dataset\n",
    "\n",
    "To fine-tune our LLM, we will use the ``trl`` library with\n",
    "the ``SFTTrainer`` class. This class expects data to be in\n",
    "a specific format outlined on [this documentation page](https://huggingface.co/docs/trl/main/en/dataset_formats#standard).\n",
    "\n",
    "The format in a nutshell is:\n",
    "- ``prompt``: The user input\n",
    "- ``completion``: The expected LLM response. In our case, this will be the name of the appropriate article category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f454fb28fbf4d62b097db3720a96ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Value\n",
    "\n",
    "# Change the label data type to string. (0 -> \"0\")\n",
    "ds['train'] = ds['train'].cast_column(\"label\", Value(dtype='string'))\n",
    "\n",
    "# Substitute all label strings with the label name. (\"0\" -> \"Company\")\n",
    "def preprocess_sample(sample, class_labels):\n",
    "   sample['label'] = class_labels[ int(sample['label'] )]\n",
    "   sample['content'] = sample['content'].strip()\n",
    "   return sample\n",
    "ds['train'] = ds['train'].map( lambda x : preprocess_sample(x, class_labels=CLASS_LABELS) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'] = ds['train'].rename_column(\"content\", \"prompt\")\n",
    "ds['train'] = ds['train'].rename_column(\"label\", \"completion\")\n",
    "ds['train'] = ds['train'].remove_columns([\"title\"])\n",
    "ds['train'] = ds['train'].shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coDi5e9WWr-K"
   },
   "source": [
    "# Load Baseline LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_MAP = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpf3SAVJWr-L"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Same quantization configuration as QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\", # QLoRA uses 4-bit NormalFloat precision,\n",
    "    bnb_4bit_use_double_quant = True, # QLoRA uses double quantising,\n",
    "    bnb_4bit_compute_dtype = torch.float32\n",
    ")\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=DEVICE_MAP)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W56z0RHRf3mL"
   },
   "outputs": [],
   "source": [
    "import transformers, torch\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=DEVICE_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View LLM GPU Usage\n",
    "By passing device_map=\"auto\", we tell 🤗 Accelerate to determine automatically where to put each layer of the model depending on the available resources:\n",
    "- first, we use the maximum space available on the GPU(s)\n",
    "- if we still need space, we store the remaining weights on the CPU\n",
    "- if there is not enough RAM, we store the remaining weights on the hard drive as memory-mapped tensors\n",
    "\n",
    "(Source: [Hugging Face](https://huggingface.co/docs/accelerate/v0.25.0/en/concept_guides/big_model_inference))\n",
    "\n",
    "However, it seems LoRA fine-tuning using 🤗 PEFT does not support multi-threading, so we will only use *one* GPU for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What GPUs are the model loaded onto?\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Mb of RAM is the model using?\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune LLM\n",
    "We will be using QLoRA to finetune the model.\n",
    "\n",
    "We will freeze the original model weights and add a small set of trainable low-rank adapter weights which will be trained via backpropagation ([Ref](https://arxiv.org/pdf/2305.14314)).\n",
    "\n",
    "When saving the finetuned model, only the adapter weights will be saved.\n",
    "\n",
    "Hugging Face has an implementation of QLoRA in the ``trl`` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LoRA Adapters\n",
    "Source: [Hugging Face](https://github.com/huggingface/smol-course/blob/main/3_parameter_efficient_finetuning/notebooks/finetune_sft_peft.ipynb), [David Godoy](https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_MODEL_NAME = \"Qwen2.5-FT-DBPedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 8 # 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 16 # 8\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05 # 0.05\n",
    "# max sequence length for model and packing of the dataset\n",
    "max_seq_length = 64\n",
    "#target_modules = 'all-linear' # all-linear TODO: Does changing this improve optimisation?\n",
    "\n",
    "peft_parameters = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    #target_modules=target_modules,  # Which modules to apply LoRA to \n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply LoRA Adapters to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the number of trainable parameters for the model.\n",
    "\n",
    "# train_p, tot_p = model.get_nb_trainable_parameters()\n",
    "# print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
    "# print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
    "# print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Class\n",
    "``SFTTrainer`` is the class used for supervised LLM finetuning in ``trl``.\n",
    "\n",
    "Source: [Hugging Face](https://github.com/huggingface/smol-course/blob/main/3_parameter_efficient_finetuning/notebooks/finetune_sft_peft.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "peft_hyperparameters = SFTConfig(\n",
    "\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,\n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=16,\n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "    # Precision settings\n",
    "    # bf16=True,  # Use bfloat16 precision TODO: Is this more optimised?\n",
    "\n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=max_seq_length,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    optim='adamw_torch_fused', #TODO: Compare with paged_adamw_8bit\n",
    "    # Learning rate schedule\n",
    "    # warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    # lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup    \n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    logging_dir='./logs',\n",
    "    output_dir=PEFT_MODEL_NAME,\n",
    "    report_to='none',\n",
    "    save_strategy=\"steps\",  # Save the model checkpoint every logging step\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    peft_config=peft_parameters,  # LoRA configuration\n",
    "    args=peft_hyperparameters,    # Hyperparameters\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 21 13:20:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:19:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8              2W /  250W |   10098MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8             21W /  250W |       4MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8             17W /  250W |       4MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:68:00.0  On |                  N/A |\n",
      "| 27%   31C    P8             21W /  250W |      12MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1819041      C   ...jl7hl-python3-3.12.8/bin/python3.12        182MiB |\n",
      "|    0   N/A  N/A   1864189      C   ...jl7hl-python3-3.12.8/bin/python3.12        182MiB |\n",
      "|    0   N/A  N/A   4055517      C   ...cqfnl-python3-3.12.8/bin/python3.12       9730MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune and save when completed\n",
    "NOTE: We will only save the *adapters* from the finetuned model, not the base model weights, so the checkpoints will be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "chunk expects at least a 1-dimensional tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(PEFT_MODEL_NAME)\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/accelerate/utils/memory.py:158\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:474\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    Compute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     (loss, outputs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# Compute token accuracy if we have labels and if the model is not using Liger (no logits)\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_liger:\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         )\n\u001b[0;32m--> 183\u001b[0m inputs, module_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_kwargs:\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:207\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     inputs: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m    204\u001b[0m     kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    205\u001b[0m     device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]],\n\u001b[1;32m    206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py:89\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Scatter with support for kwargs dictionary.\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m scattered_inputs \u001b[38;5;241m=\u001b[39m scatter(inputs, target_gpus, dim) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 89\u001b[0m scattered_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scattered_inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_kwargs):\n\u001b[1;32m     91\u001b[0m     scattered_inputs\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m     92\u001b[0m         () \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scattered_kwargs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_inputs))\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py:66\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [obj \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py:62\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py:58\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:104\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    102\u001b[0m         _get_stream(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus\n\u001b[1;32m    103\u001b[0m     ]\n\u001b[0;32m--> 104\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/nix/store/aq3rnbmaqc7kzigjbm7a7lmqxbbxdy0g-python3-3.12.8-env/lib/python3.12/site-packages/torch/nn/parallel/comm.py:205\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: chunk expects at least a 1-dimensional tensor"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(PEFT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Finetuned LLM\n",
    "Source: [Hugging Face](https://github.com/huggingface/smol-course/blob/main/3_parameter_efficient_finetuning/notebooks/finetune_sft_peft.ipynb)\n",
    "\n",
    "**REQUIREMENTS**:\n",
    "- You must already have fine-tuned the LLM (see *Finetune LLM*).\n",
    "- You must load the base model (see *Load LLM*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_MODEL_NAME = \"Qwen2.5-FT-DBPedia\"\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8Un3PF5fr-1"
   },
   "source": [
    "# Evaluate LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qQAMgasWr-H"
   },
   "outputs": [],
   "source": [
    "# Zero-shot prompt to classify articles and return their category ID.\n",
    "PROMPT_ZEROSHOT = \"\"\"You are an expert in classifying articles into categories.\n",
    "Your task is to read an article, decide which category it belongs into, and then return the number of that category.\n",
    "There are 14 categories you may choose from, but you can only decide one category.\n",
    "\n",
    "CATEGORIES:\n",
    "0. Company\n",
    "1. Educational Institution\n",
    "2. Artist\n",
    "3. Athlete\n",
    "4. Office Holder\n",
    "5. Method Of Transportation\n",
    "6. Building\n",
    "7. Natural Place\n",
    "8. Village\n",
    "9. Animal\n",
    "10. Plant\n",
    "11. Album\n",
    "12. Film\n",
    "13. Written Work\n",
    "\n",
    "Read the following article and return the most suitable category as a number (\"0\"), NOT as text (\"Company\").\n",
    "\"\"\"\n",
    "\n",
    "# Zero-shot chain-of-thought prompt to classify articles and return their category name.\n",
    "PROMPT_COT = \"\"\"You are an expert at classifying articles into the following categories:\n",
    "\n",
    "CATEGORIES:\n",
    "0. Company\n",
    "1. Educational Institution\n",
    "2. Artist\n",
    "3. Athlete\n",
    "4. Office Holder\n",
    "5. Method Of Transportation\n",
    "6. Building\n",
    "7. Natural Place\n",
    "8. Village\n",
    "9. Animal\n",
    "10. Plant\n",
    "11. Album\n",
    "12. Film\n",
    "13. Written Work\n",
    "\n",
    "Read the following article and explain which category describes its content best.\n",
    "End your answer with the category name.\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "# Meta prompt - A type of zero-shot prompt which prioritises abstract reasoning over concrete examples.\n",
    "# Advantages: Fewer tokens. Disadvantages:\n",
    "# https://www.promptingguide.ai/techniques/meta-prompting\n",
    "PROMPT_META = \"\"\"Problem: [excerpt from an encyclopedia article]\n",
    "\n",
    "Solution Structure:\n",
    "1. Begin the response with \"Let's think step by step\".\n",
    "2. Identify the subject of the encyclopedia article with \"This encyclopedia article is about [subject]\".\n",
    "3. Define what the subject is. Is it natural or artificial? Is it one or multiple entities? Use \"[subject] is a [classification]\".\n",
    "4. Consider the following list of categories:\n",
    "\t- Company\n",
    "\t- Educational Institution\n",
    "\t- Artist\n",
    "\t- Athlete\n",
    "\t- Office Holder\n",
    "\t- Method Of Transportation\n",
    "\t- Building\n",
    "\t- Natural Place\n",
    "\t- Village\n",
    "\t- Animal\n",
    "\t- Plant\n",
    "\t- Album\n",
    "\t- Film\n",
    "\t- Written Work\n",
    "   Identify all categories in this list whose properties do not match the subject.\n",
    "5. Identify which category has the most in common with the subject and explain why.\n",
    "6. Finally, state \"Category: [best matching category].\"\n",
    "\"\"\"\n",
    "\n",
    "# Few-shot chain-of-thought prompt to classify articles and return their category name.\n",
    "PROMPT_COT_4SHOT = \"\"\"Read an encyclopedia article excerpt and give it one of the following categories:\n",
    "\n",
    "CATEGORIES:\n",
    "- Company\n",
    "- Educational Institution\n",
    "- Artist\n",
    "- Athlete\n",
    "- Office Holder\n",
    "- Method Of Transportation\n",
    "- Building\n",
    "- Natural Place\n",
    "- Village\n",
    "- Animal\n",
    "- Plant\n",
    "- Album\n",
    "- Film\n",
    "- Written Work\n",
    "\n",
    "Problem:\n",
    "The Petlyakov VI-100 (Visotnyi Istrebitel – high altitude fighter) was a fighter/dive bomber aircraft designed and built in the USSR from 1938.\n",
    "\n",
    "Solution:\n",
    "Let's think step by step. This encyclopedia article is about the Petlyakov VI-100, which is an aircraft. While aircrafts are a man-made structure designed and built for a specific purpose, they are not human habitats with walls and a ceiling, so Building is not the category. Aircrafts are designed to transport people, so Method Of Transportation is the best category. Category: Method Of Transportation.\n",
    "\n",
    "Problem:\n",
    "Kruszewo [kruˈʂɛvɔ] is a village in the administrative district of Gmina Żuromin within Żuromin County Masovian Voivodeship in east-central Poland.\n",
    "\n",
    "Solution:\n",
    "Let's think step by step. This encyclopedia article is about Kruszewo, which is a village in Poland. While villages do exist within geographical areas, they are man-made, so Natural Place is not the category. While villages do contain buildings, they are not a single building, so Building is not the category. The most matching category is therefore Village. Category: Village.\n",
    "\n",
    "Problem:\n",
    "Schismus is a genus of grass in the Poaceae family. They are native to Africa and Asia.\n",
    "\n",
    "Solution:\n",
    "Let's think step by step. This encyclopedia article is about Schismus, which is a biological species. The genus of the species is grass. Grass is commonly found in natural places, but Schismus is not a geographical location, so Natural Place is not the category. Grass is a type of plant, so Plant is the most fitting category. Category: Plant.\n",
    "\n",
    "Problem:\n",
    "The Southern Oklahoma Cosmic Trigger Contest is a soundtrack by The Flaming Lips to the Bradley Beesley fishing documentary Okie Noodling.\n",
    "\n",
    "Solution:\n",
    "Let's think step by step. This encyclopedia article is about The Southern Oklahoma Cosmic Trigger Contest, which is a soundtrack to a fishing documentary. While the article mentions a fishing documentary, it is not the subject, so Film is not the category. While the article mentions the band The Flaming Lips, they are not the subject, so Artist is not the category. The most suitable category is therefore Album. Category: Album.\n",
    "\n",
    "Problem:\n",
    "\"\"\"\n",
    "\n",
    "# Few-shot chain-of-thought prompt to classify articles and return their category name.\n",
    "PROMPT_COT_2SHOT = \"\"\"Read an encyclopedia article excerpt and give it one of the following categories:\n",
    "\n",
    "CATEGORIES:\n",
    "- Company\n",
    "- Educational Institution\n",
    "- Artist\n",
    "- Athlete\n",
    "- Office Holder\n",
    "- Method Of Transportation\n",
    "- Building\n",
    "- Natural Place\n",
    "- Village\n",
    "- Animal\n",
    "- Plant\n",
    "- Album\n",
    "- Film\n",
    "- Written Work\n",
    "\n",
    "Problem:\n",
    "The Petlyakov VI-100 (Visotnyi Istrebitel – high altitude fighter) was a fighter/dive bomber aircraft designed and built in the USSR from 1938.\n",
    "\n",
    "Solution:\n",
    "Let's think step by step. This encyclopedia article is about the Petlyakov VI-100, which is an aircraft. While aircrafts are a man-made structure designed and built for a specific purpose, they are not human habitats with walls and a ceiling, so Building is not the category. Aircrafts are designed to transport people, so Method Of Transportation is the best category. Category: Method Of Transportation.\n",
    "\n",
    "Problem:\n",
    "Schismus is a genus of grass in the Poaceae family. They are native to Africa and Asia.\n",
    "\n",
    "Solution:\n",
    "Let's think step by step. This encyclopedia article is about Schismus, which is a biological species. The genus of the species is grass. Grass is commonly found in natural places, but Schismus is not a geographical location, so Natural Place is not the category. Grass is a type of plant, so Plant is the most fitting category. Category: Plant.\n",
    "\n",
    "Problem:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gq0uPzxWr-I"
   },
   "outputs": [],
   "source": [
    "def get_classification_prompt(article, prompt):\n",
    "    \"\"\"\n",
    "      For a given article in the Dataset,\n",
    "      return a LLM prompt in chat template form\n",
    "      to get its category.\n",
    "\n",
    "      Args:\n",
    "          article (Dictionary): Any item in the dataset.\n",
    "          prompt (str): A model prompt with article classification instructions.\n",
    "\n",
    "      Returns:\n",
    "          prompt (Dictionary): The prompt as a [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating).\n",
    "    \"\"\"\n",
    "    return [\n",
    "      {\"role\": \"system\", \"content\": prompt},\n",
    "      {\"role\": \"user\", \"content\": article[\"content\"].strip()},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNLzAeHbR3X9"
   },
   "outputs": [],
   "source": [
    "LABEL_NAMES=[\n",
    "    \"Company\",\n",
    "    \"Educational Institution\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"Office Holder\",\n",
    "    \"Method Of Transportation\",\n",
    "    \"Building\",\n",
    "    \"Natural Place\",\n",
    "    \"Village\",\n",
    "    \"Animal\",\n",
    "    \"Plant\",\n",
    "    \"Album\",\n",
    "    \"Film\",\n",
    "    \"Written Work\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_8az0xNY88d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "LABEL_NAMES=[\n",
    "    \"Company\",\n",
    "    \"Educational Institution\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"Office Holder\",\n",
    "    \"Method Of Transportation\",\n",
    "    \"Building\",\n",
    "    \"Natural Place\",\n",
    "    \"Village\",\n",
    "    \"Animal\",\n",
    "    \"Plant\",\n",
    "    \"Album\",\n",
    "    \"Film\",\n",
    "    \"Written Work\"\n",
    "]\n",
    "\n",
    "def get_class_label_from_string(string, class_labels = LABEL_NAMES):\n",
    "    \"\"\"\n",
    "    Extract a class label by name from a string and return its ID.\n",
    "    If no match is found, choose a random label ID.\n",
    "\n",
    "    Args:\n",
    "    string (str): A string containing the name of one class label.\n",
    "\n",
    "    Returns:\n",
    "    class_id (int): The ID of the matching class label.\n",
    "    \"\"\"\n",
    "    string = string.lower().strip()\n",
    "\n",
    "    # Concatenate all label names using boolean OR.\n",
    "    match = \"|\".join(LABEL_NAMES).lower().replace(\" \", r\"\\s*\")\n",
    "\n",
    "    # Find all instances of label name strings within the base string.\n",
    "    matches = re.findall(match, string)\n",
    "\n",
    "    # If no class label is found in the LLM text, pick a random label.\n",
    "    if matches == []:\n",
    "        print(f\"No class label found in string: {string}\")\n",
    "        return random.randint(0, len(LABEL_NAMES) - 1)\n",
    "\n",
    "    # Get the last matching label from the string.\n",
    "    final_match = matches[-1]\n",
    "\n",
    "    # Remove all capitalisation, non-alphabetic characters, and whitespace\n",
    "    labels_sanitised = [re.sub(\"[^a-z]\", \"\", label.lower()) for label in LABEL_NAMES]\n",
    "    match_sanitised = re.sub(\"[^a-z]\", \"\", final_match.lower())\n",
    "\n",
    "    # Find the matching class ID for the label.\n",
    "    class_id = labels_sanitised.index(match_sanitised)\n",
    "    return class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3Dt-uW3dmLj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_first_number_from_string(string):\n",
    "  \"\"\"\n",
    "  Returns the first whole number from a string as an integer.\n",
    "  \"\"\"\n",
    "  first_number=re.findall(r\"\\d+\",string)\n",
    "  if first_number is not None:\n",
    "    first_number = int(first_number[0])\n",
    "    return first_number\n",
    "  else:\n",
    "    raise Exception(f\"No number found in string: {string}\")\n",
    "\n",
    "def get_category_label(article, classification_prompt, extractor_func, max_tokens = 10):\n",
    "  \"\"\"\n",
    "  For a given article in the DBPedia dataset, predict its category label.\n",
    "\n",
    "  Args:\n",
    "    article (str): Article contents as raw text.\n",
    "    classifiction_prompt (str): Model instructions on how to classify articles.\n",
    "    extractor_func (func): A method which takes the model's response and returns a classification label as an integer.\n",
    "    max_tokens (int): Model response word limit.\n",
    "\n",
    "  Returns:\n",
    "    output (tuple<int, str>): The category of the article and the raw LLM output.\n",
    "  \"\"\"\n",
    "  input = get_classification_prompt(article, classification_prompt)\n",
    "\n",
    "  chat_history = pipeline(\n",
    "      input,\n",
    "      do_sample=True,\n",
    "      #top_k=10,\n",
    "      #num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "      max_new_tokens=max_tokens,\n",
    "      temperature=0.001\n",
    "      #continue_final_message=continue_final_message\n",
    "  )\n",
    "\n",
    "  response = chat_history[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "  class_id = extractor_func(response)\n",
    "\n",
    "  return (class_id, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xhhly4zjscN"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def predict_classes(dataset, classification_prompt, extractor_func, max_tokens):\n",
    "  \"\"\"\n",
    "    For a given RFPedia dataset, use the contents of each article to predict its label.\n",
    "\n",
    "    Args:\n",
    "      dataset (Dataset): The dataset to sample.\n",
    "      classifiction_prompt (str): Model instructions on how to classify articles.\n",
    "      extractor_func (func): A method which takes the model's response and returns a classification label as an integer.\n",
    "      max_tokens (int): Model response word limit.\n",
    "\n",
    "    Returns:\n",
    "      results (tuple<list, list, list>):\n",
    "        y_pred (list<int>): Predicted labels\n",
    "        y_true (list<int>): Actual labels (groundtruth)\n",
    "        responses (list<str>): Raw LLM response for each test sample.\n",
    "  \"\"\"\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "  responses = []\n",
    "\n",
    "  # TODO: This is vastly unoptimized, use a dataset for this.\n",
    "  for item in tqdm(dataset, \"Classifying articles\"):\n",
    "\n",
    "    pred_label, response = get_category_label(item, classification_prompt, extractor_func, max_tokens)\n",
    "\n",
    "    y_pred.append( pred_label )\n",
    "    y_true.append( item['label' ])\n",
    "    responses.append( response )\n",
    "\n",
    "  return y_pred, y_true, responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJmzrHaLg3YL"
   },
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhwWWSqog7a7"
   },
   "outputs": [],
   "source": [
    "configurations = [\n",
    "    # {\"name\" : \"Zero-shot\",\n",
    "    #  \"prompt\" : PROMPT_ZEROSHOT,\n",
    "    #  \"max_tokens\" : 10,\n",
    "    #  \"extractor_func\" : get_first_number_from_string},\n",
    "\n",
    "    # {\"name\" : \"Chain-of-Thought\",\n",
    "    #  \"prompt\" : PROMPT_COT,\n",
    "    #  \"max_tokens\" : 100,\n",
    "    #  \"extractor_func\" : get_class_label_from_string},\n",
    "\n",
    "    {\"name\" : \"Meta Prompt\",\n",
    "     \"prompt\" : PROMPT_META,\n",
    "     \"max_tokens\" : 100,\n",
    "     \"extractor_func\" : get_class_label_from_string},\n",
    "\n",
    "    {\"name\" : \"2-Shot CoT\",\n",
    "     \"prompt\" : PROMPT_COT_2SHOT,\n",
    "     \"max_tokens\" : 100,\n",
    "     \"extractor_func\" : get_class_label_from_string},\n",
    "\n",
    "    {\"name\" : \"4-Shot CoT\",\n",
    "     \"prompt\" : PROMPT_COT_4SHOT,\n",
    "     \"max_tokens\" : 100,\n",
    "     \"extractor_func\" : get_class_label_from_string}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gx668V-G5ay"
   },
   "outputs": [],
   "source": [
    "# Predict all article categories in the dataset\n",
    "\n",
    "y_true = None\n",
    "\n",
    "for config in tqdm(configurations, \"Testing LLM configurations\"):\n",
    "\n",
    "  args = config['prompt'], config['extractor_func'], config['max_tokens']\n",
    "\n",
    "  config['y_pred'], config['y_true'], config['responses'] = predict_classes(ds['test'], *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4Ib_XV9GKIr"
   },
   "source": [
    "## Return Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0095MWNzlFp"
   },
   "source": [
    "### Accuracy report + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmkxjb4_GLBN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_model(config):\n",
    "  \"\"\"\n",
    "  Evaluate a model configuration and save the results to output_dir.\n",
    "  \"\"\"\n",
    "\n",
    "  output_dir = os.path.join(\"output\", config['name'].replace(' ', '_').lower())\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "  y_true, y_pred, config_name = config['y_true'], config['y_pred'], config['name']\n",
    "  print(f\"\\n{config_name} evaluation results\\n\")\n",
    "\n",
    "  # Get precision, recall, and F1 score\n",
    "  classif_report = classification_report(y_true, y_pred, zero_division=0.0, output_dict=True)\n",
    "  classif_report = pd.DataFrame(classif_report).transpose()\n",
    "\n",
    "  print(classif_report)\n",
    "\n",
    "  # Save classification report to output dir\n",
    "  classif_report.to_csv( os.path.join(output_dir, \"evaluation.csv\"), escapechar='\\\\' )\n",
    "\n",
    "  # Display confusion matrix\n",
    "  try:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, y_pred,\n",
    "        display_labels = CLASS_LABELS,\n",
    "        cmap = plt.cm.Blues,\n",
    "        xticks_rotation='vertical')\n",
    "  except Exception as e:\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, y_pred,\n",
    "        cmap = plt.cm.Blues,\n",
    "        xticks_rotation='vertical')\n",
    "\n",
    "  disp.ax_.set_title(config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URb1XhrrZbrF"
   },
   "outputs": [],
   "source": [
    "for config in configurations:\n",
    "  evaluate_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edIfaY5Qzm6c"
   },
   "source": [
    "### Save all incorrect LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-035VXHEzsoR"
   },
   "outputs": [],
   "source": [
    "def get_incorrect_answers(config):\n",
    "  output_dir = os.path.join(\"output\", config['name'].replace(' ', '_').lower())\n",
    "\n",
    "  # Get a boolean mask for every incorrect prediction\n",
    "  incorrect = np.array(config['y_pred']) != np.array(config['y_true'])\n",
    "\n",
    "  # Get the index of every incorrect prediction\n",
    "  index = np.array(list(range(ds['test'].num_rows)))[incorrect]\n",
    "\n",
    "  # Get every incorrect predicted label\n",
    "  incorrect_labels = np.array(config['y_pred'])[incorrect]\n",
    "\n",
    "  incorrect_answers = {\n",
    "      # Obtain title and content of article\n",
    "      \"Title\" : np.array([item['title'] for item in ds['test']])[incorrect],\n",
    "      \"Content\" : np.array([item['content'] for item in ds['test']])[incorrect],\n",
    "\n",
    "      # Get the class names for y_pred and y_true\n",
    "      \"Predicted Category\" : [CLASS_LABELS[id] for id in np.array(config['y_pred'])[incorrect]],\n",
    "      \"Actual Category\" : [CLASS_LABELS[id] for id in np.array([item['label'] for item in ds['test']])[incorrect]],\n",
    "\n",
    "      # Get LLM raw text output\n",
    "      \"LLM Output\" : np.array(config['responses'])[incorrect]\n",
    "  }\n",
    "\n",
    "  incorrect_answers = pd.DataFrame(incorrect_answers, index=index)\n",
    "\n",
    "  incorrect_answers.to_csv( os.path.join(output_dir, \"incorrect_answers.csv\"), escapechar='\\\\' )\n",
    "\n",
    "  return incorrect_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Uj7W6fSLE7Q"
   },
   "outputs": [],
   "source": [
    "for config in configurations:\n",
    "  get_incorrect_answers(config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
